<!doctype html>
<html>

<head>
  <title>Deep Learning for Video Streaming</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container">
    <div class="menu">
      <div class="menu-table flex-row-space-between">
        <div class="logo flex-row-center">
          <a href="index.html">Deep Learning for Video Streaming</a>
        </div>
        <a class="menu-button" tabindex="0" href="javascript:void(0)">
          <img src="img/menu.png">
        </a>
        <div class="menu-items flex-row-center flex-item">
          <a href="index.html" class="menu-index">Overview</a>
          <a href="https://www.diigo.com/profile/nmenard" class="menu-index">Library Diigo</a>
        </div>
      </div>
    </div>
  </div>
  <div class="content-container">
    <div class="content">
      <div class="content-table flex-column">
        <!-------------------------------------------------------------------------------------------->
        <!--Start Intro-->
        <!--End Intro-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text Only-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Introduction</h2>
            <hr>
            <p class="text">
              Le streaming vidéo sur Internet (diffusion en continue d'une vidéo) a connu une croissance très importante depuis les dernières années, et représente
              une quantité d'information colossale.
            </p>
            <p class="text text-center graph-title">
              Nombre d’heures de vidéos uploadées par minute sur Youtube (2007-2019)
            </p>
            <img class="image center max-width-700 add-top-margin-small" src="img/Hours.PNG">
            <h3>Pourquoi utiliser le Deep Learning?</h3>
            <p class="text">
              Le Deep Learning est l'utilisation de méthodes d'apprentissage automatique comme les réseaux de neurones profonds afin de tenter d'apprendre à un niveau d'abstraction élevé des données
              et en extraire des informations.
              Le Deep Learning est très bien adapté à des données visuelles, comme des images ou des vidéos (plusieures avancées dans le domaine de la vision par ordinateur peuvent maintenant
              permettre de reconnaitre en temps réel sur un flux vidéo des objets).
              Cette synthèse de veille a pour but de montrer comment l'utilisation des méthodes de Deep Learning peuvent permettre une amélioration du streaming de vidéo sur Internet.
            </p>
            <h3>De quel type de streaming de vidéo parlons-nous?</h3>
            <p class="text">
              2 types majeurs de streaming de vidéos se distinguent:
            </p>
            <ul>
              <li>Les Vidéos à la Demande / VoD (Youtube, Dailymotion, Netflix etc...)</li>
              <li>Le Live streaming (Twitch, Mixer etc...)</li>
            </ul>
            <p class="text">
              Le Live streaming présente plus de contraintes que les VoD, du fait de la diffusion en direct de la vidéo.
              Certaines améliorations détaillés dans cette veille sont uniquement applicables aux VoD, alors que certaines utilisent les particularités du Live streaming pour tirer encore plus d'informations (en particulier en se servant du chat en direct).
            </p>
          </div>
        </div>
        <!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text with Buttons-->
        <!--End Text with Buttons-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text with Images and Image buttons-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Améliorer la QoE du visionnage de la vidéo</h2>
            <hr>
            <h3>Qu'est ce que la QoE?</h3>
            <p class="text">
              La 'Quality Of Experience' (QoE) est une mesure de la qualité de l'experience fournie par le service.
              Dans le contexte du streaming de vidéo, elle est synnonyme de confort de visionnage (Bonne résolution, bon bitrate, peu ou pas de buffering etc...).
            </p>
            <h3>Comment le Deep Learning peut permettre d'améliorer la QoE?</h3>
            <p class="text">
              La QoE lors du streaming de vidéo est grandement dépendante de la bande passante disponible du client (une mauvaise ou instable bande passante génère plus de buffering et induit une résolution d'image plus faible).
              La QoE varie donc beaucoup selon la qualité de la connexion Internet du client. 
              La très grande majorité du streaming de vidéo est faite à partir d’un PC ou d’un smartphone du côté client, lesquels disposent de capacités de calculs non utilisées lors du visionnage de la vidéo.
              Il est donc possible d'utiliser la puissance de calcul coté client afin d'améliorer la qualité de l'expérience (principalement en améliorant la résolution).
            </p>
            <h3>Réseaux de neurones de super-résolution 'Content-aware'</h3>
            <p class="text">
              Un réseau de neurones de super-résolution crée une image de haute-résolution à partir d'une image source de basse-résolution.
              Ces réseaux sont entrainés à partir de base de données d'images de basse et hautes-résolution.
              Dans le contexte de VoD, l'entrainement peut être fait de manière plus spécifique: Pour une vidéo donnée, le réseau est entrainé uniquement sur les images de la vidéo.
              Le but est d'utiliser le phénomène d'overfitting, normalement néfaste, afin d'augmenter l'efficacité du réseau.
              Cela signifie cependant qu'un réseau unique doit être entrainé pour chaque vidéo de la plateforme, ce qui représente un coût de calcul et de stockage non négligeable.
              Ci-dessous un exemple d'utilisation d'un réseau de super-résolution transformant l'image source (d) en une image de résolution supérieure (c) ou (b), respectivement sans et avec overfitting.
            </p>
            <img class="image center max-width-700 add-top-margin-small" src="img/superresolution.png">
            <h3>Réseau de neurones 'scalable'</h3>
            <p class="text">
              L'environnement de visionnage d'une vidéo est très hétérogène parmis les clients, ce qui est également le cas de la puissance de calcul disponible.
              L'inférence de l'image de haute-résolution doit se faire plus rapidement que la fréquence d'image de la vidéo (30/60Hz habituellement).
              Si la solution n'utilisait qu'un réseau fixe par vidéo, la puissance de calcul requise serais fixe également, ce qui soit sous-utilise les ressources disponibles, soit ne satisfait pas la contrainte d'inférence en temps réel.
            </p>
            <p class="text">
              C'est pourquoi le réseau doit s'adapter à la puissance de calcul disponible grâce à une structure 'scalable': le réseau est entrainé de telle manière qu'il fonctionne même certaines couches manquantes ('by-passed'). On peut ainsi à la fois créer un réseau plus simple (et ainsi plus rapide) pour des système à puissance de calcul limité, et également commencer à utiliser le réseau même si l'intégralité des poids n'ont pas été téléchargés (le réseau peut continuer d'être télécharger pendant la lecture de la vidéo).
            </p>
            <img class="image center max-width-700 add-top-margin-small" src="img/architecture.png">
            <h3>Implémentation en conjonction avec l'ABR</h3>
            <p class="text">
              L'Adaptative Bitrate est une technique visant à adapter le bitrate (résolution) de la vidéo en fonction de la bande passante disponible dans le but d'éviter du buffering tout en ayant la qualité la plus grande possible.
              Cette technique est souvent implémentée par un algorithme d'apprentissage par renforcement (comme par exemple sur Youtube).
              Pour notre solution, l'algorithme d'apprentissage par renforcement doit maintenant gérer le téléchargement des poids du réseau en plus du choix du bitrate du prochain segment vidéo à télécharger.
            </p>
            <h3>Structure de la solution</h3>
            <p class="text">
              Le fonctionnement de la solution peut être synthétisé comme suit:
            </p>
            <h4>Lorsqu'une vidéo est uploadée sur la plateforme</h4>
            <ul>
              <li>La vidéo est encodée dans plusieurs bitrates et séparée en différents 'chunks' (comportement basique)</li>
              <li>Plusieurs réseaux profonds 'content-aware' sont entraînés pour la vidéo pour l'amélioration coté client (les poids initiaux correspondent déjà à un état entraîné général pour facilité l'apprentissage)</li>
              <li>Un fichier manifeste pour la vidéo est créé, détaillant la structure des réseaux entraînés et l'adresse des fichier de poids de chaque réseaux</li>
            </ul>
            <h4>Lors de la lecture d'une vidéo</h4>
            <ul>
              <li>Le fichier manifeste est téléchargé</li>
              <li>Le client détermine le réseau adapté à sa puissance de calcul (test à partir de poids nuls pour tester le temps d'inférence)</li>
              <li>L'ABR décide ensuite à chaque étape si des couches de réseaux sont téléchargés, ou des segments vidéo (dépends de la bande passante, de la taille du buffer etc...)</li>
              <li>Quand un segment vidéo est téléchargé, il est placé dans le buffer et est passé dans le réseau de super-résolution et utilise le nombre maximum de couche permettent une inférence en temps réel</li>
              <li>Une fois l'inférence du segment vidéo terminé, le nouveau segment remplace l'ancien dans le buffer</li>
            </ul>
            <img class="image center max-width-700 add-top-margin-small" src="img/exemple.png">
            <h3>Adaptation à un contexte de Live streaming</h3>
            <p class="text">
              Cette méthode d'amélioration de la QoE nécessite d'entraîner le réseau au préalable sur la vidéo, chose impossible dans le contexte du Live streaming.
              Même s'il est toujours possible d'utiliser un réseau général de super-résolution dans ce cas, une solution plus intélligente et adaptée au Live streaming est possible:
            </p>
            <p class="text">
              Dans la plupart des utilisations de Live-streaming, la diffusion est catégorisée selon son contenu (le jeu vidéo diffusé par exemple).
              Il est donc possible d'entraîner un réseau "Content-aware" sur le sujet de la diffusion en direct.
              Cependant les performances seront moins intéressantes que dans le cas des VoDs.
            </p>
          </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Evaluer la QoE de la diffusion</h2>
            <hr>
            <h3>Intérêt et contraintes de l'évaluation de la QoE</h3>
            <p class="text">
              Savoir évaluer la QoE de la diffusion en temps réel est très important dans plusieurs situations, comme par exemple dans la solution précédente où cette évaluation est nécessaire dans l'algorithme d'Adaptative Bitrate (ABR), ou encore pour réagir en temps réel auprès de l'utilisateur (notification du problème et conseil pour améliorer la situation: proposition d'activer l'ABR si désactivé par exemple).
            </p>
            <p class="text">
              Cependant, cette estimation est complexe et chronophage, ce qui empêche toute mesure précise de la QoE en temps réel. Le but ici est d'utiliser le deep learning non supervisé afin d'obtenir une solution rapide et polyvalente à différents types de vidéo.
            </p>
            <h3>Machine de Boltzmann Restreinte (RBM) et apprentissage non-supervisé</h3>
            <p class="text">
              L'apprentissage non-supervisé permet d'inférer une structure dans des données non labélisées.
              Une Machine de Boltzmann restreinte est un type de réseau d'apprentissage non-supervisé, adapté à l'estimation de densité et permet d'apprendre la distribution de probabilité de ses inputs.
            </p>
            <p class="text">
              L'apprentissage a pour objectif ici de minimiser l'erreur entre les inputs et les inputs reconstruits.
            </p>
            <p class="text">
              Ainsi, quand le réseau rencontre des inputs ne faisant pas partie de la distribution apprise lors de la phase d'apprentissage, l'erreur entre les iputs et les inputs reconstruits augmente.
            </p>
            <p class="text">
              La dégradation de la vidéo peut donc être estimée par l'erreur entre les inputs de la vidéo coté client et ces mêmes inputs reconstruits à partir du réseau entrainé sur la vidéo originale.
            </p>
            <h3>Principe de la méthode</h3>
            <p class="text">
              Les inputs associés à une séquence vidéos sont des features corrélées à la QoE:
            </p>
            <ul>
              <li>Le bitrate</li>
              <li>Le nombre de frames reçues</li>
              <li>La complexité de la scène ainsi que le mouvement, empiriquement extrait de l'encodage</li>
              <li>La pixelisation</li>
              <li>Le ratio de bruit</li>
              <li>Le flou moyen</li>
              <li>L'intensité du mouvement (au niveau du pixel)</li>
            </ul>
            <p class="text">
              Ces features ont été choisies car elles sont récupérables en temps réel et présentent une corrélation élevée avec la qualité de la vidéo.
            </p>
            <h4>La méthode fonctionne donc de la manière suivante:</h4>
            <ul>
              <li>Le modèle RBM est entrainé avec les vidéos disponibles sur la plateforme</li>
              <li>Quand le client ouvre une session, il télécharge le modèle</li>
              <li>Une fois que le streaming commence, le client extrait en direct les 8 features mentionnées précedemment</li>
              <li>Quand la séquence vidéo (2-10 secondes) est terminée, ces features sont moyennées, insérées dans le modèle RMB, et ainsi reconstruites</li>
              <li>L'erreur RMSE entre les features et leur versions reconstruites donne une estimation de la dégradation de la vidéo</li>
            </ul>
            <img class="image center max-width-700 add-top-margin-small" src="img/QoE.PNG">
            <p class="text">
              Cette méthode donne une corrélation entre 78 et 91% avec la méthode VQM (Video Quality Metric), qui est une métrique adaptée à la vision humaine.
            </p>
          </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin">Génération automatique de temps forts</h2>
            <hr>
            <h3>Contexte</h3>
            <p class="text">
              <img class="image image-wrap-text max-width-400" src="img/chat.gif">
              Un avantage que le Live streaming a comparé aux VoDs est la présence d'un chat instantané permettant d'obtenir la réaction du public lors d'une diffusion en direct.
            </p>


          </div>
        </div>
        <!--End Credits-->
        <!-------------------------------------------------------------------------------------------->
      </div>
    </div>
  </div>
</body>

</html>